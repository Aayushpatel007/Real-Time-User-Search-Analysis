{"cells":[{"cell_type":"markdown","source":["###Real Time User Search Query Analysis Using Spark Structured Streaming, Delta Lake and Amazon Kinesis\n#####Final Project: AMOD-5410 -Big Data\n#####Team Members: \n\nAayush Patel - 0672897, \n\nJay Trivedi, \n\nPrateek Rajput"],"metadata":{}},{"cell_type":"markdown","source":["**Problem:** A client/company have developed their website and wants to analyze and store their user's search query to analyze what users are looking for and generate daily reports which can help them so solve several business questions. \n\nThis project will demonstrate how to use Spark Structured Streaming, Delta lake and Kinesis to store and analyze data in the best possible way. Additionally, we would also cover several Spark configurational details which is very important for writing an optimized code. \n\nOverall Stack of components include: \n* **Apache Spark Structured Streaming**\n* **Delta Lake - Developed by Databricks**\n* **Amazon Kinesis DataStreams**\n* **AWS Glue - Data Catlog**\n* **AWS Kinesis Firehouse**\n* **AWS Kinesis DataStreams**\n* **AWS S3**\n* **Django - Python Web Devlopement Framework**"],"metadata":{}},{"cell_type":"markdown","source":["**Architecture 1: **\n\n<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Diagram+1.png\" width=\"560\" height=\"480\" style=\"vertical-align:center;\"/>\n\nProducers:\n\n==> We have our website deployed on AWS EC2 instance which is build using Django. \n\n<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Screenshot+from+2020-04-13+08-48-52.png\" width=\"560\" height=\"480\" style=\"vertical-align:center;\"/>\n\n**Why Kinesis streams and what are its configurations:**\n\n==> KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. \n\n==> Shards: Currently we have used only one shard. (Shard is basically the base throughput unit of Kinesis. One shard provides a capacity of 1MB/sc data input and 2MB/sec data output. To get more throughput units we can initalize more shards)\n\n**Consumers: **\n\n1. Spark Structured Streaming Job - Perform transformations and aggregations for data analysis\n2. Kinesis Firehouse \n\n**Why do we need Kinesis Firehose?**\n\nNow, let us suppose our streaming job is cancelled or a cluster is down due to any reason and we have multiple jobs running up. Eg. One writestream() for storing records and another writestreams which performs several queries for data analysis. Hence, Kinesis Firehose is the easiest way to reliably load streaming data into data lakes, data stores and analytics tools. Finally, this stored data into S3 in parquet format with defaault compression- Snappy\n\n[https://docs.aws.amazon.com/firehose/latest/dev/record-format-conversion.html]\n\n\n**Why AWS GLUE? **\n\nIn big data world, we always want to store data in the most effective way. File formats like JSON/CSV are not recommended if we are using SparK OR Hive to perform analysis. Hence, in our case as we are dealing with Spark we transform all records from JSON to Parquet format and store data partitioned by Date. (ORC can be used when using Hive)\n\n**Configurations and code for Spark structured streaming will be covered in further sections.**"],"metadata":{}},{"cell_type":"markdown","source":["####The most exciting and interesting Part of this project \"Welcome to the world of Data Lakehouse - Delta Lake\"\n\n<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Data+Lakehouse.jpg\" width=\"660\" height=\"580\" style=\"vertical-align:center;\"/>\n\n**Problems when using Data Lake(Amazon S3), or even HDFS. **\n* Schema enforcement when new tables are introduced \n* Table repairs when any new data is inserted into the data lake\n* Frequent refreshes of metadata \n* Bottlenecks of small file sizes for distributed computations\n* Difficulty re-sorting data by an index (i.e. userID) if data is spread across many files and partitioned by i.e. eventTime\n\n**Delta Lake to the rescue**\n\n* First of all, it is **10-100x faster than spark on Parquet.**\n* <b>ACID transactions</b> - Multiple writers can simultaneously modify a data set and see consistent views.\n* <b>DELETES/UPDATES/UPSERTS</b> - Writers can modify a data set without interfering with jobs reading the data set.\n* <b>Automatic file management</b> - Data access speeds up by organizing data into large files that can be read efficiently.\n* <b>Statistics and data skipping</b> - Reads are 10-100x faster when statistics are tracked about the data in each file, allowing Delta to avoid reading irrelevant information.\n\n**Architecture 2**\n\n\n<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Diagram+2.png\" width=\"560\" height=\"480\" style=\"vertical-align:center;\"/>\n\n**OPTIMIZATION Mechanisms to speed up queries**\n\n<b>Partition Pruning</b> is a performance optimization that speeds up queries by limiting the amount of data read.\n\n<b>Data Skipping</b> is a performance optimization that aims at speeding up queries that contain filters (WHERE clauses). \n\n<b>ZOrdering</b> is a technique to colocate related information in the same set of files. ZOrdering maps multidimensional data to one dimension while preserving locality of the data points. Algorithm: [https://en.wikipedia.org/wiki/Z-order_curve]\n\n<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Untitled+Diagram+(13).png\" width=\"560\" height=\"480\" style=\"vertical-align:center;\"/>"],"metadata":{}},{"cell_type":"markdown","source":["###Getting Started \n\n* Step 1: Clone this Github repository for getting code for website. [https://github.com/Aayushpatel007/Real-Time-User-Search-Analysis]\n\n* Step 2: Create a Kinesis stream on AWS with as many shards as you want according to your usecase. \n\n* Step 3: Create an AWS Secret access key and Access key from IAM. - My security credentials\n\n* Step 4. Configure aws-cli on your machine for storing aws credentials.\n\n* Step 4. Under views.py replace kinesis_stream variable and the region respectively.\n\n* Step 5. Optional = Create a Kinesis Firehose for the input source as \"Kinesis Data Streams\" and sink as \"Amazon S3\". \n\n* Step 6. Optional = Create a Glue table for transforming data to parquet/orc partitoned by \"Date\". \n\n* Step 7. Use the below Spark structured streaming code to analyze data. \n\nNote: Instructions for setting up and using Delta lake is given below."],"metadata":{}},{"cell_type":"markdown","source":["**Spark Structured Streaming Code: **\n\nBefore going through the code, lets change the default number of paritions(sparks' default parallelism) = 8. Hence for performance optimization, we can lower the no of partitions. As, **Too few partitions** – Cannot utilize all cores available in the cluster. **Too many partitions** – Excessive overhead in managing many small tasks."],"metadata":{}},{"cell_type":"code","source":["sc.defaultParallelism\n# spark.conf.set(\"spark.default.parallelism\",2)\n#We set default parallelism to \"2\" as there are two cores and we don't want to more than  2 partitions to avoid internal shuffle and sort. Changing and adjusting this parameter can make a huge difference when there is a huge cluster. Also shuffle.partitons =200 , we cand change that value to tune performance"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: 8</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["**Using readstream to read from kinesis datastream**"],"metadata":{}},{"cell_type":"code","source":["kinesisDF = spark \\\n  .readStream \\\n  .format(\"kinesis\") \\\n  .option(\"streamName\", \"search-queries-ingestion\") \\\n  .option(\"initialPosition\", \"earliest\") \\\n  .option(\"region\", \"us-east-1\") \\\n  .option(\"awsAccessKey\", \"\") \\\n  .option(\"awsSecretKey\", \"\") \\\n  .load()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["**Creating a schema**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\npythonSchema = (StructType() \n          .add(\"Query\", StringType()) \n          .add(\"Timestamp\", StringType()) \n          .add(\"Name\", StringType()) \n          .add (\"No_of_words\", IntegerType()) \n          .add (\"Browser\", StringType()) \n          .add(\"Device\", StringType()) \n          .add (\"Date\", StringType()))\n\n# If you want only one writestream which stores data in memory and then perform several queries on top of that table\n\n\"\"\"\ndataDF = (kinesisDF \n  .selectExpr(\"cast (data as STRING) jsonData\") \n  .select(from_json(\"jsonData\", pythonSchema).alias(\"data\")) \n  .select(\"data.*\") \n  .writeStream \n  .format(\"memory\")\n  .outputMode(\"append\")\n  .queryName(\"searchdata\")\n  .start())\n\"\"\"  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["**If you want to see all incoming data from Kinesis stream - (Need to uncomment the code above with dataDF)**"],"metadata":{}},{"cell_type":"code","source":["#Just for debugging and checking whether data comes from kinesis\nfrom time import sleep\nwhile True:\n  spark.sql(\"SELECT * FROM searchdata\").show()\n  sleep(2)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["**Finding top 5 users who are most active on the website and search data**"],"metadata":{}},{"cell_type":"code","source":["Max_queries_per_user = (kinesisDF.selectExpr(\"cast (data as STRING) jsonData\") \n  .select(from_json(\"jsonData\", pythonSchema).alias(\"data\")) \n  .select(\"data.Name\") \n  .groupBy(\"Name\").count().sort(desc(\"count\")).limit(5))\ndisplay(Max_queries_per_user)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>count</th></tr></thead><tbody><tr><td>Aayush</td><td>5</td></tr><tr><td>Patel</td><td>1</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["**Performing what \"word\" users searches the most. Note stop words have been removed before sending data from website using NLTK**"],"metadata":{}},{"cell_type":"code","source":["words = (kinesisDF.selectExpr(\"cast (data as STRING) jsonData\") \n  .select(from_json(\"jsonData\", pythonSchema).alias(\"data\")) \n  .select(\"data.Query\")\n  .select(\n  #explode turns each item in an array into a separate row\n  explode(\n        split(\"Query\", ' ')\n       ).alias('word')))      \n#Generate a running word count\nwordCounts = words.groupBy('word').count().sort(desc(\"count\")).limit(10)\ndisplay(wordCounts)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>word</th><th>count</th></tr></thead><tbody><tr><td>aayush</td><td>1</td></tr><tr><td>name</td><td>1</td></tr><tr><td>hello</td><td>4</td></tr><tr><td>great</td><td>1</td></tr><tr><td>spark</td><td>1</td></tr><tr><td>jsdhksjdkjskdj</td><td>1</td></tr><tr><td>world</td><td>3</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["**Genreating User Profile**"],"metadata":{}},{"cell_type":"code","source":["#Generating Individual Report \nUser_report_df = (kinesisDF.selectExpr(\"cast (data as STRING) jsonData\") \n  .select(from_json(\"jsonData\", pythonSchema).alias(\"data\")) \n  .select(\"data.Name\")\n  .where(\"Name == aayush\"))\n\nno_of_queries = User_report_df.count()\navg_no_of_words_per_query = User_report_df.select(avg(User_report_df[\"No_of_words\"]))\nunique_browser = User_report_df.select(\"Browser\").distinct()\nunique_device = User_report_df.select(\"Device\").distinct()\ntop_5_words = (User_report_df.select(\"data.Query\")\n  .select(\n  #explode turns each item in an array into a separate row\n  explode(\n        split(lines.value, ' ')\n       ).alias('word'))  )    \n#Generate a running word count\nwordCounts = top_5_words.groupBy('word').withColumnRenamed(\"count\",\"distinct_words\").sort(desc(\"count\"))\n\nprint(\"No of queries:\", no_of_queries)\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/glue.png\" width=\"660\" height=\"580\" style=\"vertical-align:center;\"/>"],"metadata":{}},{"cell_type":"markdown","source":["### Delta Lake implementation\n\nBefore starting implementation, we need to know the configuration setting and packages required to use delta lake. \n\nS3 as a storage system:\n\n```\nbin/pyspark \\\n --packages io.delta:delta-core_2.11:0.2.0, \\\n            org.apache.hadoop:hadoop-aws:2.7.7 \\\n --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore \\\n --conf spark.hadoop.fs.s3a.access.key=<your-s3-access-key> \\\n --conf spark.hadoop.fs.s3a.secret.key=<your-s3-secret-key>\n```\n\nHDFS as a storage system:\n\n```\nspark.delta.logStore.class=org.apache.spark.sql.delta.storage.HDFSLogStore\n```\n\nAzure Data Lake Gen1 as a storage system\n\n```\nbin/spark-shell \\\n  --packages io.delta:delta-core_2.11:0.2.0,org.apache.hadoop:hadoop-azure-datalake:2.9.2 \\\n  --conf spark.delta.logStore.class=org.apache.spark.sql.delta.storage.AzureLogStore \\\n  --conf spark.hadoop.dfs.adls.oauth2.access.token.provider.type=ClientCredential \\\n  --conf spark.hadoop.dfs.adls.oauth2.client.id=<your-oauth2-client-id> \\\n  --conf spark.hadoop.dfs.adls.oauth2.credential=<your-oauth2-credential> \\\n  --conf spark.hadoop.dfs.adls.oauth2.refresh.url=https://login.microsoftonline.com/<your-directory-id>/oauth2/token\n```\n\n**Checkpointing**\n\nPath: /delta/events/_checkpoints/searchdata\n\nIf the job fails due to some reason, we don't have to read or write the data again."],"metadata":{}},{"cell_type":"markdown","source":["**Writing to a delta lake**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n#ACCESS_KEY = dbutils.secrets.get(scope = \"aws\", key = \"\")\n#SECRET_KEY = dbutils.secrets.get(scope = \"aws\", key = \"\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"\")\nsc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"\")\npythonSchema = (StructType() \n          .add(\"Query\", StringType()) \n          .add(\"Timestamp\", StringType()) \n          .add(\"Name\", StringType()) \n          .add (\"No_of_words\", IntegerType()) \n          .add (\"Browser\", StringType()) \n          .add(\"Device\", StringType()) \n          .add (\"Date\", StringType()))\n\ndataDevicesDF = (kinesisDF \n  .selectExpr(\"cast (data as STRING) jsonData\") \n  .select(from_json(\"jsonData\", pythonSchema).alias(\"data\")) \n  .select(\"data.*\") \n  .writeStream \n  .format(\"delta\")\n .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/delta/events/_checkpoints/searchdata\")\n  .start(\"/mnt/delta/data\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["#delta_stream = spark.readStream.format(\"delta\").load(\"/mnt/delta/data\")\n#Now we can use the same code above to perform several analysis."],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["<img src=\"https://bigdataprojectaayush.s3.amazonaws.com/Delta-Lake-marketecture-0423c.png\" width=\"660\" height=\"580\" style=\"vertical-align:center;\"/>"],"metadata":{}}],"metadata":{"name":"demo","notebookId":2433359897424953},"nbformat":4,"nbformat_minor":0}
